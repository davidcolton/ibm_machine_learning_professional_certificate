{"cells":[{"cell_type":"markdown","id":"4692078a-8179-4542-9a74-ba20cb0321cf","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 3, Part b: K-Nearest Neighbor LAB\n"]},{"cell_type":"markdown","id":"a27a3088-f2a6-465c-a7ec-ef1eb6551014","metadata":{},"outputs":[],"source":["## Introduction and Learning Goals\n","\n","In this lab, we will explore classification using the K-Nearest Neighbors approach. We will use a customer churn dataset from the telecom industry, which includes customer data such as long-distance usage, data usage, monthly revenue, types of offerings, and other services purchased by customers. The data, based on a fictional telecom firm, includes several Excel files which have been combined and are available in the course materials. We are using the subset of customers who have phone accounts. Since the data includes a mix of numeric, categorical, and ordinal variables, we will load this data and do some preprocessing. Then we will use K-nearest neighbors to predict customer churn rates.\n","\n","After completing this lab, you should have a working understanding of how to preprocess a variety of variables to apply the K-Nearest Neighbors algorithm, understand how to choose K, and understand how to evaluate model performance.\n"]},{"cell_type":"code","id":"0a7ff4fb-8f5b-4ed5-9609-62c18b42e03d","metadata":{},"outputs":[],"source":["!pip install pandas\n!pip install numpy\n!pip install matplotlib\n!pip install seaborn\n!pip install scikit-learn"]},{"cell_type":"code","id":"fa220207-78d8-4f19-a4e2-44fe98582f1f","metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt, os, sys, seaborn as sns"]},{"cell_type":"markdown","id":"93b72483-045c-4e49-9372-35645b5d26df","metadata":{},"outputs":[],"source":["## Question 1\n","\n","* We begin by importing the data. Examine the columns and data.\n","* Notice that the data contains a unique ID, an indicator for phone customer status, total lifetime value, total revenue, and a bank-estimated churn score. We will not be using these features, so they can be dropped from the data.\n","* Begin by taking an initial look at the data, including both numeric and non-numeric features.\n"]},{"cell_type":"code","id":"7b0ec211-242b-4d2b-9ffe-a5a05a7e937f","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n\ndf = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/churndata_processed.csv\")"]},{"cell_type":"code","id":"1eeea3d6-e297-4c54-a28b-0d3a29526b20","metadata":{},"outputs":[],"source":["round(df.describe(),2)"]},{"cell_type":"markdown","id":"cfa09b9c-4a73-444c-bc7d-2e5b640b40c0","metadata":{},"outputs":[],"source":["## Question 2\n","\n","* Identify which variables are binary, categorical and not ordinal, categorical and ordinal, and numeric.  The non-numeric features will need to be encoded using methods we have discussed in the course.\n","* Start by identifying the number of unique values each variable takes, then create list variables for categorical, numeric, binary, and ordinal variables. \n","* Note that the variable 'months' can be treated as numeric, but it may be more convenient to transform it to an ordinal variable.\n","* For the other categorical variables, examine their values to determine which may be encoded ordinally.\n"]},{"cell_type":"code","id":"2b4ecf06-12d1-465d-8dbd-bb05de9016e4","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndf_uniques"]},{"cell_type":"code","id":"b3a3e91f-b6ca-439b-8a05-8c5d53a810a8","metadata":{},"outputs":[],"source":["binary_variables = list(df_uniques[df_uniques['Unique Values'] == 2].index)\nbinary_variables"]},{"cell_type":"code","id":"bb58bc3d-66df-48de-8efe-2159cc25b422","metadata":{},"outputs":[],"source":["categorical_variables = list(df_uniques[(6 >= df_uniques['Unique Values']) & (df_uniques['Unique Values'] > 2)].index)\ncategorical_variables"]},{"cell_type":"code","id":"740118f2-80f4-4fd1-88f5-ba9392c2fcc1","metadata":{},"outputs":[],"source":["[[i, list(df[i].unique())] for i in categorical_variables]"]},{"cell_type":"code","id":"41390611-ae09-40dc-86d6-74d8bf8d2354","metadata":{},"outputs":[],"source":["ordinal_variables = ['contract', 'satisfaction']"]},{"cell_type":"code","id":"7d699af3-1025-46ff-88ae-669fe7e77a91","metadata":{},"outputs":[],"source":["df['months'].unique()"]},{"cell_type":"code","id":"9feb928b-de6c-4939-ab30-2389e5b13c96","metadata":{},"outputs":[],"source":["ordinal_variables.append('months')"]},{"cell_type":"code","id":"34593d9b-e9e1-4698-a950-e9dc0fcd2e55","metadata":{},"outputs":[],"source":["numeric_variables = list(set(df.columns) - set(ordinal_variables) - set(categorical_variables) - set(binary_variables))"]},{"cell_type":"code","id":"5987e940-bf22-4a62-9736-dc136d24782e","metadata":{},"outputs":[],"source":["df[numeric_variables].hist(figsize=(12, 6))"]},{"cell_type":"code","id":"e6f3cda4-7fc2-4f23-8a75-e89f6bdf658e","metadata":{},"outputs":[],"source":["df['months'] = pd.cut(df['months'], bins=5)\n### END SOLUTION"]},{"cell_type":"markdown","id":"fb1c479f-580e-49bc-bc1d-11816d96f23b","metadata":{},"outputs":[],"source":["## Question 3\n","* Having set up the variables, remember that the K-nearest neighbors algorithm uses distance and hence requires scaled data. \n","* Scale the data using one of the scaling methods discussed in the course.\n","* Save the processed dataframe as a comma-separated file: 'churndata_processed.csv'\n"]},{"cell_type":"code","id":"88c87e97-983d-4c2b-9d05-5628f7c64a5d","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder"]},{"cell_type":"code","id":"edba036c-e51a-4926-ac73-13014e74496e","metadata":{},"outputs":[],"source":["lb, le = LabelBinarizer(), LabelEncoder()"]},{"cell_type":"code","id":"1e482fc1-c830-421b-9864-62f104f3c5d3","metadata":{},"outputs":[],"source":["for column in ordinal_variables:\n    df[column] = le.fit_transform(df[column])"]},{"cell_type":"code","id":"670e9395-ae0e-4fbd-b5ee-1001e419331e","metadata":{},"outputs":[],"source":["df[ordinal_variables].astype('category').describe()"]},{"cell_type":"code","id":"3a31e952-e059-4587-a407-0c1a2d638cbb","metadata":{},"outputs":[],"source":["for column in binary_variables:\n    df[column] = lb.fit_transform(df[column])"]},{"cell_type":"code","id":"ef28932f-9817-4cd2-843a-1f9ab7976cb4","metadata":{},"outputs":[],"source":["categorical_variables = list(set(categorical_variables) - set(ordinal_variables))"]},{"cell_type":"code","id":"3ed53513-0037-49a1-b913-bb41e6d04147","metadata":{},"outputs":[],"source":["df = pd.get_dummies(df, columns = categorical_variables, drop_first=True)"]},{"cell_type":"code","id":"c6e1bd7b-ab97-481d-acf9-f2e7cdfb1e51","metadata":{},"outputs":[],"source":["df.describe().T"]},{"cell_type":"code","id":"080bf6e9-b5ea-4bcd-b2d5-1227fdce20d2","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()"]},{"cell_type":"code","id":"2caf8514-a839-49f6-9309-0f39d783781d","metadata":{},"outputs":[],"source":["for column in [ordinal_variables + numeric_variables]:\n    df[column] = mm.fit_transform(df[column])"]},{"cell_type":"code","id":"9b5115e3-48aa-4caf-979f-aba3fc307e99","metadata":{},"outputs":[],"source":["round(df.describe().T, 3)"]},{"cell_type":"code","id":"21bd2d22-ca0e-422b-98ad-f794b4f9a227","metadata":{},"outputs":[],"source":["### END SOLUTION\n\n# Save a copy of the processed data for later use\noutputfile = 'churndata_processed.csv'\ndf.to_csv(outputfile, index=False)"]},{"cell_type":"markdown","id":"e2e7c655-0aae-4209-a7b7-46fde396d199","metadata":{},"outputs":[],"source":["## Question 4\n","* Now that the data are encoded and scaled, separate the features (X) from the target (y, churn_value). \n","* Split the sample into training and test samples, with the test sample representing 40% of observations.\n","* Estimate a K-Nearest Neighbors model, using K=3.\n","* Examine the Precision, Recall, F-1 Score, and Accuracy of the classification.\n","* Use a graphic to illustrate the Confusion Matrix. \n"]},{"cell_type":"code","id":"655dbfcd-de54-47b6-b0d6-0501fea7b2b1","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score"]},{"cell_type":"code","id":"75880b2a-a788-48c8-bb6f-3279f34949b8","metadata":{},"outputs":[],"source":["# Set up X and y variables\ny, X = df['churn_value'], df.drop(columns='churn_value')\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"]},{"cell_type":"code","id":"159e0491-1ce1-4f68-a245-a7e9ef753adc","metadata":{},"outputs":[],"source":["# Estimate KNN model and report outcomes\nknn = KNeighborsClassifier(n_neighbors=3)\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n# Preciision, recall, f-score from the multi-class support function\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))"]},{"cell_type":"code","id":"49281aa5-5e62-4424-a311-9db1cd3d9c1a","metadata":{},"outputs":[],"source":["# Plot confusion matrix\nsns.set_palette(sns.color_palette())\n_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['False', 'True']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels[::-1], fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION"]},{"cell_type":"markdown","id":"3fe32c72-b9fa-4691-8530-7cca5fc6055a","metadata":{},"outputs":[],"source":["## Question 5\n","* Using the same split of training and test samples, estimate another K-Nearest Neighbors model.\n","* This time, use K=5 and weight the results by distance.\n","* Again, examine the Precision, Recall, F-1 Score, and Accuracy of the classification, and visualize the Confusion Matrix. \n"]},{"cell_type":"code","id":"40d3c5a0-7bf9-418a-b950-f0663b877ea7","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nknn = KNeighborsClassifier(n_neighbors=5, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n# Preciision, recall, f-score from the multi-class support function\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))"]},{"cell_type":"code","id":"0e9a0104-cf52-46c9-8161-36684538d906","metadata":{},"outputs":[],"source":["# Plot confusion matrix\n_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['False', 'True']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels[::-1], fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION"]},{"cell_type":"markdown","id":"33d71da6-624c-42f9-a869-6cfba7a9d057","metadata":{},"outputs":[],"source":["## Question 6\n","* To determine the right value for K, examine results for values of K from 1 to 40.\n","* This time, focus on two measures, the F-1 Score, and the Error Rate (1-Accuracy).\n","* Generate charts which plot each of these measures as a function of K. \n","* What do these charts suggest about the optimal value for K?\n"]},{"cell_type":"code","id":"9f3b25ca-1553-4c1d-bc16-24b9e1922824","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nmax_k = 40\nf1_scores = list()\nerror_rates = list() # 1-accuracy\n\nfor k in range(1, max_k):\n    \n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn = knn.fit(X_train, y_train)\n    \n    y_pred = knn.predict(X_test)\n    f1 = f1_score(y_pred, y_test)\n    f1_scores.append((k, round(f1_score(y_test, y_pred), 4)))\n    error = 1-round(accuracy_score(y_test, y_pred), 4)\n    error_rates.append((k, error))\n    \nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nerror_results = pd.DataFrame(error_rates, columns=['K', 'Error Rate'])"]},{"cell_type":"code","id":"726683f4-0d76-4327-9329-baa9027b6fab","metadata":{},"outputs":[],"source":["# Plot F1 results\nsns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = f1_results.set_index('K').plot(figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.title('KNN F1 Score')\nplt.savefig('knn_f1.png')"]},{"cell_type":"code","id":"b1f8f5e1-c551-4bad-9e43-339cfce0dceb","metadata":{},"outputs":[],"source":["# Plot Accuracy (Error Rate) results\nsns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = error_results.set_index('K').plot(figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='Error Rate')\nax.set_xticks(range(1, max_k, 2))\nplt.title('KNN Elbow Curve')\nplt.savefig('knn_elbow.png')\n### END SOLUTION"]},{"cell_type":"markdown","id":"b6f25409-f33d-4d99-bcac-a43d7d45dd8f","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"706ee59602ed5f8bdcdd73335795d1edaeda8f91c4f2ecd47fb663a8a03267ee"},"nbformat":4,"nbformat_minor":4}