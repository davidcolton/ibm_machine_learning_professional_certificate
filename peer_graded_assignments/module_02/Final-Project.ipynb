{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7917a707-0287-4d5d-864d-5e067f95d6ce",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">Supervised Machine Learning: Regression - Final Assignment</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ff34a-ca99-41a9-9655-a7579ee28775",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "In this Assignment, you will demonstrate the data regression skills you have learned by completing this course. You are expected to leverage a wide variety of tools, but also this report should focus on present findings, insights, and next steps. You may include some visuals from your code output, but this report is intended as a summary of your findings, not as a code review. \n",
    "\n",
    "The grading will center around 5 main points:\n",
    "\n",
    "1. Does the report include a section describing the data?\n",
    "2. Does the report include a paragraph detailing the main objective(s) of this analysis? \n",
    "3. Does the report include a section with variations of linear regression models and specifies which one is the model that best suits the main objective(s) of this analysis.\n",
    "4. Does the report include a clear and well-presented section with key findings related to the main objective(s) of the analysis?\n",
    "5. Does the report highlight possible flaws in the model and a plan of action to revisit this analysis with additional data or different predictive modeling techniques? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter Black for cell formatting\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c9045-0373-4723-883d-bc02c51b236a",
   "metadata": {},
   "source": [
    "## Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8830e-64f5-4155-a96d-12706f5e3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats.mstats import normaltest\n",
    "\n",
    "# Importing Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    cross_val_predict,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    "    LeaveOneOut,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Lasso, Ridge\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39cd1c-c5aa-4057-8f86-4281a7d921ee",
   "metadata": {},
   "source": [
    "## Importing the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af3dbc-f38c-4290-b556-2c574aba0405",
   "metadata": {},
   "source": [
    "Before you begin, you will need to choose a data set that you feel passionate about. You can brainstorm with your peers about great public data sets using the discussion board in this module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67a342-6c9c-4d1c-9fda-8c2380e570e4",
   "metadata": {},
   "source": [
    "Once you have selected a data set, you will produce the deliverables listed below and submit them to one of your peers for review. Treat this exercise as an opportunity to produce analysis that are ready to highlight your analytical skills for a senior audience, for example, the Chief Data Officer, or the Head of Analytics at your company.\n",
    "Sections required in your report:\n",
    "\n",
    "*   Main objective of the analysis that specifies whether your model will be focused on prediction or interpretation.\n",
    "*   Brief description of the data set you chose and a summary of its attributes.\n",
    "*   Brief summary of data exploration and actions taken for data cleaning and feature engineering.\n",
    "*   Summary of training at least three linear regression models which should be variations that cover using a simple  linear regression as a baseline, adding polynomial effects, and using a regularization regression. Preferably, all use the same training and test splits, or the same cross-validation method.\n",
    "*  A paragraph explaining which of your regressions you recommend as a final model that best fits your needs in terms of accuracy and explainability.\n",
    "*  Summary Key Findings and Insights, which walks your reader through the main drivers of your model and insights from your data derived from your linear regression model.\n",
    "*  Suggestions for next steps in analyzing this data, which may include suggesting revisiting this model adding specific data features to achieve a better explanation or a better prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1ee90-8963-428b-bfa9-f9691135df84",
   "metadata": {},
   "source": [
    "# 1. About the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This dataset will consider Vinho Verde, a unique product from the Minho (north-west) region of Portugal. Medium in alcohol, is it particularly appreciated due\n",
    "to its freshness (specially in the summer). This wine accounts for 15% of the total Portuguese production, and around 10% is exported, mostly white wine.\n",
    "\n",
    "Once viewed as a luxury good, nowadays wine is increasingly enjoyed by a wider range of consumers. Portugal is a top ten wine exporting country with 3.17% of the market share in 2005. Exports of its vinho verde wine (from the northwest region) have increased by 36% from 1997 to 2007. To support its growth, the wine industry is investing in new technologies for both wine making and selling processes. Wine certification and quality assessment are key elements within this context. Certification prevents the illegal adulteration of wines (to safeguard human health) and assures quality for the wine market. Quality evaluation is often part of the certification process and can be used to improve wine making (by identifying the most influential factors) and to stratify wines such as premium brands (useful for setting prices).\n",
    "\n",
    "Wine certification is generally assessed by physicochemical and sensory tests. Physicochemical laboratory tests routinely used to characterize wine include determination of density, alcohol or pH values, while sensory tests rely mainly on human experts. It should be stressed that taste is the least understood of the human senses, thus wine classification is a difficult task. Moreover, the relationships between the physicochemical and sensory analysis are complex and still not fully understood.\n",
    "\n",
    "Advances in information technologies have made it possible to collect, store and process massive, often highly complex datasets. All this data hold valuable information such as trends and patterns, which can be used to improve decision making and optimize chances of success. \n",
    "\n",
    "[Source](https://repositorium.sdum.uminho.pt/bitstream/1822/10029/1/wine5.pdf)\n",
    "\n",
    "# 2. Main Objective\n",
    "\n",
    "In this project the objective of the analysis will be focused on prediction of the quality of the wine.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The dataset is available [here](http://www3.dsi.uminho.pt/pcortez/wine/) or [here](https://archive.ics.uci.edu/dataset/186/wine+quality).\n",
    "\n",
    "| Variable Name        | Role    | Type        | Description / Comment                                                                                                                                                                                                 |\n",
    "|----------------------|---------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| fixed acidity        | Feature | Continuous  | Fixed acidity in wine is primarily due to the presence of stable acids such as tartaric, malic, citric, and succinic. These acids contribute to the overall taste and balance of the wine.                            |\n",
    "| volatile acidity     | Feature | Continuous  | Often referred to as VA, volatile acidity is a measure of a wineâ€™s gaseous acids. The amount of VA in wine is often considered an indicator of spoilage.                                                              |\n",
    "| citric acid          | Feature | Continuous  | Citric acid is often added to wines to increase acidity, complement a specific flavour or prevent ferric hazes.                                                                                                       |\n",
    "| residual sugar       | Feature | Continuous  | Residual Sugar (or RS) is from natural grape sugars leftover in a wine after the alcoholic fermentation finishes.                                                                                                     |\n",
    "| chlorides            | Feature | Continuous  | Chlorides in wine are salts of mineral acids that can affect the taste and quality of the wine.                                                                                                                       |\n",
    "| free sulfur dioxide  | Feature | Continuous  | Free sulfur dioxide (SO2) is a preservative for wine. It has both antioxidant and antimicrobial properties, making it an effective preservative                                                                       |\n",
    "| total sulfur dioxide | Feature | Continuous  | Total Sulfur Dioxide (TSO2) in wine is the portion of SO2 that is free in the wine plus the portion that is bound to other chemicals in the wine such as aldehydes, pigments, or sugars.                              |\n",
    "| density              | Feature | Continuous  | Wine density is determined by the amount of sugar, alcohol, and other solutes present in the wine. Generally, the higher the sugar and alcohol content, the higher the density of the wine.                           |\n",
    "| pH                   | Feature | Continuous  | The acidity of the wine. The pH of wine typically ranges from about 2.9 to 4.0. White wine usually has a pH level of 3.0 to 3.4, while red wine is between 3.3 to 3.6                                                 |\n",
    "| sulphates            | Feature | Continuous  | Sulfates, or sulfur dioxide (SO2), are naturally occurring compounds that have been used in winemaking for centuries. They are a type of preservative that can help prevent oxidation and microbial spoilage in wine. |\n",
    "| alcohol              | Feature | Continuous  | Wine alcohol content varies depending on the type of wine and the amount poured. A standard serving of wine is 5 ounces and generally contains between 11-13% alcohol by volume                                       |\n",
    "| colour               | Other   | Categorical | red or white                                                                                                                                                                                                          |\n",
    "| quality              | Target  | Integer     | score between 0 and 10                                                                                                                                                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the red wine and white wine data and concatenate together\n",
    "df_red = pd.read_csv(\"./data/winequality-red.csv\", sep=\";\")\n",
    "df_red[\"colour\"] = \"red\"\n",
    "\n",
    "df_white = pd.read_csv(\"./data/winequality-white.csv\", sep=\";\")\n",
    "df_white[\"colour\"] = \"white\"\n",
    "\n",
    "df = pd.concat([df_red, df_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the dataframe\n",
    "data_shape = df.shape\n",
    "print(f\"The data has {data_shape[0]} rows of data and {data_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ead6fc-8a60-438e-8cc7-17ee519a99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 10 rows of data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa0821-6041-42a8-ad7a-04aad56d308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dtypes of each row\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40c6cc-34fc-4737-b464-fc8991c87d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spread of the data we can see that:\n",
    "\n",
    "* Chlorides range is 0.009 to 0.611\n",
    "* Total Sulphur Dioxide range is 6.0 to 440.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that there are no null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a significant number, 1177, of duplicate values in the data. There is no indication in the documentation with the data that there is actually any duplication / up-sampling of the data so, for the purposes of this project, it will be assumed that these duplicates are actual real data examples that just happen to have the same values. Given the repeatable wine making techniques, processes and raw materials this is highly probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness and kurtosis\n",
    "\n",
    "### Skewness:\n",
    "\n",
    "Skewness is a statistical term and it is a way to estimate or measure the shape of a distribution.  It is an important statistical methodology that is used to estimate the asymmetrical behavior rather than computing frequency distribution. Skewness can be two types:\n",
    "\n",
    "1. Symmetrical: A distribution can be called symmetric if it appears the same from the left and right from the center point.\n",
    "2. Asymmetrical: A distribution can be called asymmetric if it doesnâ€™t appear the same from the left and right from the center point.\n",
    "\n",
    "Distribution on the basis of skewness value:\n",
    "\n",
    "* Skewness = 0: Then normally distributed.\n",
    "* Skewness > 0: Then more weight in the left tail of the distribution.\n",
    "* Skewness < 0: Then more weight in the right tail of the distribution.\n",
    "\n",
    "### Kurtosis:\n",
    "\n",
    "Is also a statistical term and an important characteristic of frequency distribution. It determines whether a distribution is heavy-tailed in respect of the normal distribution. It provides information about the shape of a frequency distribution.\n",
    "\n",
    "* Kurtosis for normal distribution is equal to 3.\n",
    "* For a distribution having kurtosis < 3: It is called playkurtic.\n",
    "* For a distribution having kurtosis > 3, It is called leptokurtic and it signifies that it tries to produce more outliers rather than the normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Skewness and kurtosis of the numeric data\n",
    "df_num = df.select_dtypes(include=\"number\")\n",
    "df_num_columns = df_num.columns\n",
    "df_num_skew = df_num.skew()\n",
    "df_num_kurt = df_num.kurtosis()\n",
    "\n",
    "df_num_summary = pd.DataFrame(\n",
    "    zip(df_num_columns, df_num_skew, df_num_kurt),\n",
    "    columns=[\"Column\", \"Skew\", \"Kurtosis\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "From the table above we can see that most measures are either positively or negatively skewed. Total Sulfur Dioxide appears to be the most normal distribution in restect to skew. Volitile Acidity is the attribute with the a Kurtosis value that appears to be most normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "for column in df_num:\n",
    "    plt.figure(figsize=(8, 0.75))\n",
    "    sns.boxplot(data=df_num, x=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Most measures have a significant number of outliers apart from **Alcohol**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying the corellations between features using Heat Map!\n",
    "plt.figure(dpi=300)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(np.round(df_num.corr(), 2), annot=True, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting features according to the strength of corretlation with Quality feature\n",
    "df_num.corr()[\"quality\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Alcohol** is has the strongest positive correlation and **density** has the highest negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Normality\n",
    "\n",
    "A target variable that is normally distributed will often lead to better linear regression model results. If our target is not normally distributed, we can apply a transformation to it and then fit our regression to predict the transformed values. \n",
    "\n",
    "How can we tell if our target is normally distributed? There are two ways:\n",
    "\n",
    "1. Plotting the Histogram\n",
    "2. Applying D'Agostino K^2 test to check the normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df[\"quality\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying D'Agostino K^2 test to check the normality!\n",
    "norm = normaltest(df[\"quality\"].values)\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The p-value is quite far away of 0.05 which indicates absense of normality! Linear Regression assumes a normally distributed residuals which can be aided by transforming the y variable (Our target).\n",
    "\n",
    "Transformations techniques to get or approach normal distribution:\n",
    "1. Square Root\n",
    "2. Log Transformation\n",
    "3. Box cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcff5f-fdae-46da-984c-b0fc7326a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the transformations\n",
    "sqrt_quality = np.sqrt(df[\"quality\"])\n",
    "sqrt_test_res = normaltest(sqrt_quality.values)\n",
    "\n",
    "log_quality = np.log(df[\"quality\"])\n",
    "log_test_res = normaltest(log_quality.values)\n",
    "\n",
    "bc_quality = boxcox(df[\"quality\"])\n",
    "boxcox_medv, lam = bc_quality\n",
    "boxcox_test_res = normaltest(boxcox_medv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7d7fb-33f1-4883-bd1f-8eedf2f43d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results\n",
    "df_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Transormation\": [\"Original\", \"Square-Root\", \"Log\", \"Box Cox\"],\n",
    "        \"P-value\": [norm[1], sqrt_test_res[1], log_test_res[1], boxcox_test_res[1]],\n",
    "    }\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Cox appears to give the best result so we will use this transformation in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(boxcox_medv);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc0326-09fb-4f5c-8727-0f79e3264f87",
   "metadata": {},
   "source": [
    "# 3. Linear Regression Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def performance_evaluation(y_true, y_pred, model, data):\n",
    "    metric_name = [\"R2\", \"MSE\", \"MAE\"]\n",
    "    metric_value = [\n",
    "        r2_score(y_true, y_pred),\n",
    "        mean_squared_error(y_true, y_pred),\n",
    "        mean_absolute_error(y_true, y_pred),\n",
    "    ]\n",
    "    df = pd.DataFrame(\n",
    "        zip(metric_name, metric_value), columns=[\"metric_name\", \"metric_value\"]\n",
    "    )\n",
    "    df[\"model\"] = model\n",
    "    df[\"data\"] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Read it clean again# Data Retrieving\n",
    "df_red = pd.read_csv(\"./data/winequality-red.csv\", sep=\";\")\n",
    "df_red[\"colour\"] = \"red\"\n",
    "\n",
    "df_white = pd.read_csv(\"./data/winequality-white.csv\", sep=\";\")\n",
    "df_white[\"colour\"] = \"white\"\n",
    "\n",
    "df = pd.concat([df_red, df_white])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b409291-10df-4d6d-993c-b7fa0a9c587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the object (string) columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "for category in categorical_cols:\n",
    "    le.fit(df[category].drop_duplicates())\n",
    "    df[category] = le.transform(df[category])\n",
    "\n",
    "# Normalise Target\n",
    "df[\"quality\"] = boxcox(df[\"quality\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f74d2d-e2f0-4316-b0fe-e2c55d3a17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df.drop(\"quality\", axis=1)\n",
    "y = df[\"quality\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create folds\n",
    "kf = KFold(shuffle=True, random_state=42, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c05734-2142-439c-835d-cc18c7dd158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"polynomial_features\", PolynomialFeatures()),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"polynomial_features__degree\": range(3),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator, params, cv=kf)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ce2a4-41e5-41da-8595-b4ba36a362a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating best model on the Training Data\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"polynomial_features\", PolynomialFeatures(include_bias=False, degree=1)),\n",
    "        (\"linear_regression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "lr_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d648a8-5b0e-4dd4-a213-0002881e7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A table to hold all results\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Training Data Results\n",
    "y_train_hat = lr_pipeline.predict(X_train)\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_train, y_train_hat, \"Linear Regression\", \"Training Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Linear Regression\")\n",
    "    & (df_results[\"data\"] == \"Training Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb29ad-a366-4678-ba45-4fd4d268b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_train, y_train_hat, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Training Data Linear Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Results\n",
    "y_test_hat = lr_pipeline.predict(X_test)\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(y_test, y_test_hat, \"Linear Regression\", \"Test Data\"),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Linear Regression\") & (df_results[\"data\"] == \"Test Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_train, y_train_hat, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Test Data Linear Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge Regression is a regression technique aimed at preventing overfitting in linear regression models when the model is too complex and fits the training data very closely, but performs poorly on new and unseen data. The algorithm adds a penalty term (referred to as the L2 regularization) to the linear regression cost function that is proportional to the square of the magnitude of the coefficients. This approach helps to reduce the magnitude of the coefficients in the model, which in turn can prevent overfitting and is especially helpful where there are many correlated predictor variables in the model. A hyperparameter alpha serving as a constant that multiplies the L2 term thereby controlling regularization strength needs to be optimized through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters for the ridge regression model\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Defining a pipeline for the  ridge regression model\n",
    "ridge_regression_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"polynomial_features\", PolynomialFeatures(include_bias=False, degree=1)),\n",
    "        (\"ridge_regression\", RidgeCV(alphas=alphas, cv=None, store_cv_results=True)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting the ridge regression model\n",
    "ridge_regression_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the optimal alpha\n",
    "ridge_regression_pipeline[\"ridge_regression\"].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Training Data\n",
    "y_train_hat_ridge = ridge_regression_pipeline.predict(X_train)\n",
    "\n",
    "performance_evaluation(y_train, y_train_hat_ridge, \"Ridge Regression\", \"Training Data\")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_train, y_train_hat_ridge, \"Ridge Regression\", \"Training Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Ridge Regression\")\n",
    "    & (df_results[\"data\"] == \"Training Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149feb9-8708-4cc0-8894-90acba85707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_train, y_train_hat_ridge, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Training Data Ridge Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Testing Data\n",
    "y_test_hat_ridge = ridge_regression_pipeline.predict(X_test)\n",
    "\n",
    "performance_evaluation(y_test, y_test_hat_ridge, \"Ridge Regression\", \"Test Data\")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_test, y_test_hat_ridge, \"Ridge Regression\", \"Test Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Ridge Regression\") & (df_results[\"data\"] == \"Test Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_test, y_test_hat_ridge, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Test Data Ridge Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (LASSO) Regression is a regression technique aimed at preventing overfitting in linear regression models when the model is too complex and fits the training data very closely, but performs poorly on new and unseen data. The algorithm adds a penalty term (referred to as the L1 regularization) to the linear regression cost function that is proportional to the absolute value of the coefficients. This approach can be useful for feature selection, as it tends to shrink the coefficients of less important predictor variables to zero, which can help simplify the model and improve its interpretability. A hyperparameter alpha serving as a constant that multiplies the L1 term thereby controlling regularization strength needs to be optimized through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters for the ridge regression model\n",
    "alphas_lasso = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Defining a pipeline for the  lasso regression model\n",
    "lasso_regression_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"polynomial_features\", PolynomialFeatures(include_bias=False, degree=1)),\n",
    "        (\"lasso_regression\", LassoCV(alphas=alphas, cv=LeaveOneOut())),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting the ridge regression model\n",
    "lasso_regression_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the optimal alpha\n",
    "lasso_regression_pipeline[\"lasso_regression\"].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Training Data\n",
    "y_train_hat_lasso = lasso_regression_pipeline.predict(X_train)\n",
    "\n",
    "performance_evaluation(y_train, y_train_hat_lasso, \"LASSO Regression\", \"Training Data\")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_train, y_train_hat_lasso, \"LASSO Regression\", \"Training Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"LASSO Regression\")\n",
    "    & (df_results[\"data\"] == \"Training Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_train, y_train_hat_lasso, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Training Data LASSO Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Testing Data\n",
    "y_test_hat_lasso = lasso_regression_pipeline.predict(X_test)\n",
    "\n",
    "performance_evaluation(y_test, y_test_hat_lasso, \"LASSO Regression\", \"Test Data\")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_test, y_test_hat_lasso, \"LASSO Regression\", \"Test Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"LASSO Regression\") & (df_results[\"data\"] == \"Test Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_test, y_test_hat_lasso, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Test Data LASSO Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "\n",
    "Elastic Net Regression is a regression technique aimed at preventing overfitting in linear regression models when the model is too complex and fits the training data very closely, but performs poorly on new and unseen data. The algorithm is a combination of the ridge and LASSO regression methods, by adding both L1 and L2 regularization terms in the cost function. This approach can be useful when there are many predictor variables that are correlated with the response variable, but only a subset of them are truly important for predicting the response. The L1 regularization term can help to select the important variables, while the L2 regularization term can help to reduce the magnitude of the coefficients. Hyperparameters alpha which serves as the constant that multiplies the penalty terms, and l1_ratio that serves as the mixing parameter that penalizes as a combination of L1 and L2 regularization - need to be optimized through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446a2d9-c5f4-4b61-8c04-21eea999bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters for the elastic-net regression model\n",
    "l1_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "alphas_en = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Defining a pipeline for the elastic-net regression model\n",
    "elasticnet_regression_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"polynomial_features\", PolynomialFeatures(include_bias=False, degree=1)),\n",
    "        (\n",
    "            \"elasticnet_regression\",\n",
    "            ElasticNetCV(alphas=alphas_en, l1_ratio=l1_ratios, cv=LeaveOneOut()),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting an elastic-net regression model\n",
    "elasticnet_regression_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad0aa3-6d01-44fa-9056-2c757b310836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the optimal alpha\n",
    "elasticnet_regression_pipeline[\"elasticnet_regression\"].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the optimal l1_ratio\n",
    "elasticnet_regression_pipeline[\"elasticnet_regression\"].l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Training Data\n",
    "y_train_hat_en = elasticnet_regression_pipeline.predict(X_train)\n",
    "\n",
    "performance_evaluation(\n",
    "    y_train, y_train_hat_en, \"Elastic Net Regression\", \"Training Data\"\n",
    ")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_train, y_train_hat_en, \"Elastic Net Regression\", \"Training Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Elastic Net Regression\")\n",
    "    & (df_results[\"data\"] == \"Training Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_train, y_train_hat_en, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Test Data Elastic Net Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the Testing Data\n",
    "y_test_hat_en = elasticnet_regression_pipeline.predict(X_test)\n",
    "\n",
    "performance_evaluation(y_test, y_test_hat_en, \"Elastic Net Regression\", \"Test Data\")\n",
    "\n",
    "df_results = pd.concat(\n",
    "    [\n",
    "        df_results,\n",
    "        performance_evaluation(\n",
    "            y_test, y_test_hat_en, \"Elastic Net Regression\", \"Test Data\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_results[\n",
    "    (df_results[\"model\"] == \"Elastic Net Regression\")\n",
    "    & (df_results[\"data\"] == \"Test Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "figure = plt.figure(figsize=(8, 5))\n",
    "axes = plt.axes()\n",
    "plt.grid(True)\n",
    "axes.plot(y_test, y_test_hat_en, marker=\"o\", ls=\"\", ms=3.0)\n",
    "lim = (1.5, 5.25)\n",
    "axes.set(\n",
    "    xlabel=\"Actual Quality\",\n",
    "    ylabel=\"Predicted Quality\",\n",
    "    xlim=lim,\n",
    "    ylim=lim,\n",
    "    title=\"Test Data Elastic Net Regression Model Prediction Performance\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99916712-f3e7-4936-8289-9babf6415306",
   "metadata": {},
   "source": [
    "# 4. Insights and key findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model was the worst performing test model among the candidate models.\n",
    "\n",
    "* $R^2$ = 0.273143\n",
    "* Mean Squared Error = 0.162492\n",
    "* Mean Absolute Error = 0.310617\n",
    "\n",
    "Among the penalized models, the optimal elastic net regression model demonstrated the best independent test model performance with the tuned hyperparameter leaning towards a lower L1 regularization effect of 0.1.\n",
    "\n",
    "* $R^2$ = 0.267927\n",
    "* Mean Squared Error = 0.163658\n",
    "* Mean Absolute Error = 0.311906\n",
    "\n",
    "The optimal lasso regression model using an L1 regularization term demonstrated a high independent test model performance.\n",
    "\n",
    "* $R^2$ = 0.272135\n",
    "* Mean Squared Error = 0.162717\n",
    "* Mean Absolute Error = 0.310927\n",
    "\n",
    "The optimal ridge regression model using an L2 regularization term equally demonstrated good independent test model performance.\n",
    "\n",
    "* $R^2$ = 0.273128\n",
    "* Mean Squared Error = 0.162495\n",
    "* Mean Absolute Error = 0.310633\n",
    "\n",
    "In all instance the models generalised well and had a slightly lower $R^2$ value on the Test Data.\n",
    "\n",
    "The computed r-squared metrics for the formulated models were all very close - only ranging from 0.267 to 0.273, which could be further improved by:\n",
    "* Considering more informative predictors\n",
    "* Considering more complex models other than linear regression and its variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f63a0e-e5f7-457f-90a1-82f552e4acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results[\"data\"] == \"Test Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7171d78-b2cd-49a9-ab95-04382e94911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results[\"metric_name\"] == \"R2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x=\"metric_value\",\n",
    "    y=\"model\",\n",
    "    hue=\"data\",\n",
    "    orient=\"h\",\n",
    "    data=df_results[df_results[\"metric_name\"] == \"R2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305cac4-ac53-4448-b3e2-2a22702d70d7",
   "metadata": {},
   "source": [
    "# 5. Next Steps\n",
    "\n",
    "The next steps would be to look at more sophisticated models e.g. SVM or to look at this challlenge as a classification problem i.e. could we predict the colour of the wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795fc77-dd35-486e-950e-9e7b5fe62e43",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "prev_pub_hash": "ecc74c8238c35f5dfa2a9a7b20110ae88a5ecc79b5a7defaa2490883e9c0f0e3"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
