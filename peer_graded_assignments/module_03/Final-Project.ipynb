{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134c9045-0373-4723-883d-bc02c51b236a",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8830e-64f5-4155-a96d-12706f5e3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter Black for cell formatting\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the red wine and white wine data and concatenate together\n",
    "df_red = pd.read_csv(\"./data/winequality-red.csv\", sep=\";\")\n",
    "df_red[\"color\"] = \"red\"\n",
    "\n",
    "df_white = pd.read_csv(\"./data/winequality-white.csv\", sep=\";\")\n",
    "df_white[\"color\"] = \"white\"\n",
    "\n",
    "df = pd.concat([df_red, df_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"color\"] = df[\"color\"].replace(\"white\", \"0\").replace(\"red\", \"1\").astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa0821-6041-42a8-ad7a-04aad56d308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data type of each row\n",
    "print(df.dtypes.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40c6cc-34fc-4737-b464-fc8991c87d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "print(df.describe().T.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that there are no null values\n",
    "print(df.isnull().sum().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")\n",
    "sns.pairplot(df, hue=\"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything except \"color\"\n",
    "fields = list(df.columns[:-1])\n",
    "correlations = df[fields].corrwith(df[\"color\"])\n",
    "correlations.sort_values(inplace=True)\n",
    "ax = correlations.plot(kind=\"bar\")\n",
    "ax.set(ylim=[-1, 1], ylabel=\"Pearson Correlation\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_columns(df: pd.DataFrame, columns: list = None) -> pd.DataFrame:\n",
    "    df_copy = df.copy()  # Avoid modifying the original DataFrame.\n",
    "\n",
    "    if columns is None:\n",
    "        numeric_cols = df_copy.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        columns = numeric_cols\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    try:\n",
    "        df_copy[columns] = scaler.fit_transform(df_copy[columns])\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: One or more specified columns not found. {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during scaling. Check if columns contain numeric data: {e}\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "df_norm = minmax_scale_columns(df, df.columns[:-1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_norm.describe().round(2).T.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "print(df_norm[\"color\"].value_counts(normalize=True).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train / test data\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, test_index in split.split(df_norm, df_norm[\"color\"]):\n",
    "    df_train = df_norm.loc[train_index]\n",
    "    df_test = df_norm.loc[test_index]\n",
    "\n",
    "# Create the Train and the Test data\n",
    "X_train = df_train[df_train.columns[:-1]]\n",
    "X_test = df_test[df_test.columns[:-1]]\n",
    "\n",
    "y_train = df_train[\"color\"]\n",
    "y_test = df_test[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts(normalize=True).to_markdown())\n",
    "print(y_test.value_counts(normalize=True).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard logistic regression\n",
    "lr = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty=\"l1\", solver=\"liblinear\").fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty=\"l2\", solver=\"liblinear\").fit(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for each model\n",
    "y_pred = list()\n",
    "\n",
    "coeff_labels = [\"lr\", \"l1\", \"l2\"]\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab, mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "\n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average=\"weighted\")\n",
    "\n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "\n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(\n",
    "        label_binarize(y_test, classes=[0, 1]),\n",
    "        label_binarize(y_pred[lab], classes=[0, 1]),\n",
    "        average=\"weighted\",\n",
    "    )\n",
    "\n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "\n",
    "    metrics.append(\n",
    "        pd.Series(\n",
    "            {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"fscore\": fscore,\n",
    "                \"auc\": auc,\n",
    "            },\n",
    "            name=lab,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_metrics = pd.concat(metrics, axis=1)\n",
    "df_metrics.index.name = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = df_metrics.transpose()\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.round(5).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display or plot the confusion matrix for each model.\n",
    "fig, axList = plt.subplots(nrows=2, ncols=3)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "axList[-1].axis(\"off\")\n",
    "\n",
    "for ax, lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt=\"d\")\n",
    "    ax.set(title=lab)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility method to evaluate the model performance.\n",
    "def evaluate_metrics(yt, yp, model_name):\n",
    "    results_pos = {}\n",
    "    precision, recall, f_beta, _ = precision_recall_fscore_support(\n",
    "        yt, yp, average=\"binary\"\n",
    "    )\n",
    "    results_pos[\"model\"] = model_name\n",
    "    results_pos[\"precision\"] = float(precision)\n",
    "    results_pos[\"recall\"] = float(recall)\n",
    "    results_pos[\"accuracy\"] = accuracy_score(yt, yp)\n",
    "    results_pos[\"fscore\"] = float(f_beta)\n",
    "    return results_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Train the model with training dataset:\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make the predictions\n",
    "svm_y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_perf = evaluate_metrics(y_test, svm_y_pred, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Probability for AUC\n",
    "model_prob = SVC(probability=True, random_state=42)\n",
    "model_prob.fit(X_train, y_train.values.ravel())\n",
    "svm_prob_y_pred = model_prob.predict(X_test)\n",
    "\n",
    "# Calculate AUC-ROC and add to results\n",
    "svm_perf[\"auc\"] = roc_auc_score(y_test, svm_prob_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model performance to the metrics dataframe\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([svm_perf]).set_index(\"model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's try `GridSearchCV` to find the optimized `C` and `kernel` combination:\n",
    "params_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100, 500],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "}\n",
    "opto_model = SVC(random_state=42)\n",
    "\n",
    "# Define a GridSearchCV to search the best parameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=opto_model,\n",
    "    param_grid=params_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    ")\n",
    "# Search the best parameters with training data\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "best_model = SVC(random_state=42, C=1, kernel=\"poly\")\n",
    "\n",
    "# Train the model with training dataset:\n",
    "best_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make the predictions\n",
    "best_model_y_pred = best_model.predict(X_test)\n",
    "\n",
    "best_model_perf = evaluate_metrics(y_test, best_model_y_pred, \"svm best\")\n",
    "\n",
    "# With Probability for AUC\n",
    "best_model_prob = SVC(probability=True, random_state=42, C=1, kernel=\"poly\")\n",
    "best_model_prob.fit(X_train, y_train.values.ravel())\n",
    "best_model_prob_y_pred = best_model_prob.predict(X_test)\n",
    "\n",
    "# Calculate AUC-ROC and add to results\n",
    "best_model_perf[\"auc\"] = roc_auc_score(y_test, best_model_prob_y_pred)\n",
    "\n",
    "# Add the model performance to the metrics dataframe\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([best_model_perf]).set_index(\"model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.round(4).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, svm_y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, best_model_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try K from 1 to 50\n",
    "max_k = 50\n",
    "\n",
    "# Create an empty list to store f1score for each k\n",
    "f1_scores = []\n",
    "\n",
    "# Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n",
    "for k in range(1, max_k + 1):\n",
    "    # Create a KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Train the classifier\n",
    "    knn = knn.fit(X_train, y_train.values.ravel())\n",
    "    preds = knn.predict(X_test)\n",
    "    # Evaluate the classifier with f1score\n",
    "    f1 = f1_score(preds, y_test)\n",
    "    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n",
    "# Convert the f1score list to a dataframe\n",
    "f1_results = pd.DataFrame(f1_scores, columns=[\"K\", \"F1 Score\"])\n",
    "f1_results.set_index(\"K\")\n",
    "\n",
    "# This is a long list and different to analysis, so let's visualize the list using a linechart.\n",
    "# Plot F1 results\n",
    "ax = f1_results.plot(figsize=(6, 4))\n",
    "ax.set(xlabel=\"Num of Neighbors\", ylabel=\"F1 Score\")\n",
    "ax.set_xticks(range(1, max_k, 5))\n",
    "plt.ylim((0.96, 1))\n",
    "plt.title(\"KNN F1 Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "# Train the classifier\n",
    "knn = knn.fit(X_train, y_train.values.ravel())\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "knn_perf = evaluate_metrics(y_test, knn_preds, \"knn\")\n",
    "\n",
    "# 5. Get predicted probabilities for the positive class (class 1)\n",
    "knn_prob = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 6. Calculate the AUC-ROC score\n",
    "knn_perf[\"auc\"] = np.nan\n",
    "\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([knn_perf]).set_index(\"model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.round(4).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, knn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "dt_preds = dt.predict(X_test)\n",
    "dt_perf = evaluate_metrics(y_test, dt_preds, \"dt\")\n",
    "dt_perf[\"auc\"] = np.nan\n",
    "\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([dt_perf]).set_index(\"model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.tree_.node_count, dt.tree_.max_depth)\n",
    "print(confusion_matrix(y_test, dt_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params_grid = {\n",
    "    \"max_depth\": range(1, dt.tree_.max_depth + 1, 2),\n",
    "    \"max_features\": range(1, len(dt.feature_importances_) + 1),\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "df_grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=df_params_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "df_best = df_grid_search.fit(X_train, y_train)\n",
    "\n",
    "df_best_preds = df_best.predict(X_test)\n",
    "df_best_perf = evaluate_metrics(y_test, df_best_preds, \"dt best\")\n",
    "df_best_perf[\"auc\"] = np.nan\n",
    "\n",
    "df_metrics = pd.concat([df_metrics, pd.DataFrame([df_best_perf]).set_index(\"model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics.round(4).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_best.best_estimator_.tree_.node_count, df_best.best_estimator_.tree_.max_depth)\n",
    "print(confusion_matrix(y_test, df_best_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "prev_pub_hash": "ecc74c8238c35f5dfa2a9a7b20110ae88a5ecc79b5a7defaa2490883e9c0f0e3"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
